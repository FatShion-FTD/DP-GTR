{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Disclose the error to the patient and put it in the operative report\n",
      "B: Tell the attending that he cannot fail to disclose this mistake\n",
      "C: Report the physician to the ethics committee\n",
      "D: Refuse to dictate the operative report\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_json = {\"A\": \"Disclose the error to the patient and put it in the operative report\", \"B\": \"Tell the attending that he cannot fail to disclose this mistake\", \"C\": \"Report the physician to the ethics committee\", \"D\": \"Refuse to dictate the operative report\" }\n",
    "prompt = \"\"\n",
    "\n",
    "for k, v in test_json.items():\n",
    "    prompt += f\"{k}: {v}\\n\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "test_df = data['test'].to_pandas()\n",
    "\n",
    "test_df = test_df.head(210)\n",
    "cnt = 0\n",
    "fin_df = pd.DataFrame(columns=['question', 'options', 'answer_idx', 'metamap_phrases'])\n",
    "for i in range(210):\n",
    "    if fin_df.shape[0] == 200:\n",
    "        break\n",
    "    if '?' in test_df.iloc[i]['question']:\n",
    "        fin_df.loc[cnt] = [test_df.iloc[i]['question'], test_df.iloc[i]['options'], test_df.iloc[i]['answer_idx'], test_df.iloc[i]['metamap_phrases']]\n",
    "        cnt += 1\n",
    "\n",
    "fin_df.to_json('dataset/medQA_4.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 4)\n",
      "                                            question  \\\n",
      "0  A junior orthopaedic surgery resident is compl...   \n",
      "1  A 67-year-old man with transitional cell carci...   \n",
      "2  Two weeks after undergoing an emergency cardia...   \n",
      "3  A 39-year-old woman is brought to the emergenc...   \n",
      "4  A 35-year-old man comes to the physician becau...   \n",
      "\n",
      "                                             options answer_idx  \\\n",
      "0  {'A': 'Disclose the error to the patient and p...          B   \n",
      "1  {'A': 'Inhibition of proteasome', 'B': 'Hypers...          D   \n",
      "2  {'A': 'Renal papillary necrosis', 'B': 'Choles...          B   \n",
      "3  {'A': 'Coagulase-positive, gram-positive cocci...          D   \n",
      "4  {'A': 'Erythromycin ointment', 'B': 'Ketotifen...          B   \n",
      "\n",
      "                                     metamap_phrases  \n",
      "0  [junior orthopaedic surgery resident, completi...  \n",
      "1  [67 year old man, transitional cell carcinoma ...  \n",
      "2  [Two weeks, emergency cardiac, stenting, unsta...  \n",
      "3  [year old woman, brought, emergency department...  \n",
      "4  [35 year old man, physician, of itchy, watery,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fin_df = pd.read_json('dataset/medQA_4.json', orient='records')\n",
    "print(fin_df.shape)\n",
    "print(fin_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lifting door is convenient for three direction travel, but it also serves as a security measure at a what?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SecurityXuanwuLab/HaS-820m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"SecurityXuanwuLab/HaS-820m\").to('cuda:0')\n",
    "hide_template = \"\"\"<s>Paraphrase the text:%s\\n\\n\"\"\"\n",
    "# original_input = \"张伟用苹果(iPhone 13)换了一箱好吃的苹果。\"\n",
    "original_input = \"A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\"\n",
    "input_text = hide_template % original_input\n",
    "inputs = tokenizer(input_text, return_tensors='pt').to('cuda:0')\n",
    "pred = model.generate(**inputs, max_length=100)\n",
    "pred = pred.cpu()[0][len(inputs['input_ids'][0]):]\n",
    "hide_input = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "print(hide_input)\n",
    "\n",
    "# output:\n",
    "# 李华用华为(Mate 20)换了一箱美味的橙子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lifting door is convenient for three direction travel, but it also serves as a security measure at a what?\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A, a loading dock, B, a parking garage, C, a car wash, D, a car dealership.\n",
      "\n",
      "I'm gonna say a loading dock.\n",
      "\n",
      "That is correct.\n",
      "\n",
      "Yes.\n",
      "\n",
      "All right, so you're at two for two. You're doing pretty good.\n",
      "\n",
      "I'm doing pretty good.\n",
      "\n",
      "All right, question number three. What is the name of the device that is used to measure the amount of light that is reflected off of a surface?\n",
      "\n",
      "A, a photometer, B, a spectrometer, C, a reflectometer, D, a radiometer.\n",
      "\n",
      "I'm gonna say a reflectometer.\n",
      "\n",
      "That is correct.\n",
      "\n",
      "Yes.\n",
      "\n",
      "All right, you're doing pretty good. You're at three for three. All right, question number four. What is the name of the device that is used to measure the amount of light that is reflected off of a surface?\n",
      "\n",
      "A, a photometer, B, a spectrometer, C, a reflectometer, D, a radiometer.\n",
      "\n",
      "I'm gonna say a reflectometer.\n",
      "\n",
      "That is correct.\n",
      "\n",
      "Yes.\n",
      "\n",
      "All\n",
      "========================================\n",
      " A, a loading dock, B, a parking garage, C, a car wash, D, a car dealership.\n",
      "\n",
      "I'm gonna say a loading dock.\n",
      "\n",
      "That is correct.\n",
      "\n",
      "Yes.\n",
      "\n",
      "All right, so you're at two for two. You're doing pretty good.\n",
      "\n",
      "I'm doing pretty good.\n",
      "\n",
      "All right, question number three. What is the name of the device that is used to measure the amount of light that is reflected off of a surface?\n",
      "\n",
      "A, a photometer, B, a spectrometer, C, a reflectometer, D, a radiometer.\n",
      "\n",
      "I'm gonna say a reflectometer.\n",
      "\n",
      "That is correct.\n",
      "\n",
      "Yes.\n",
      "\n",
      "All right, you're doing pretty good. You're at three for three. All right, question number four. What is the name of the device that is used to measure the amount of light that is reflected off of a surface?\n",
      "\n",
      "A, a photometer, B, a spectrometer, C, a reflectometer, D, a radiometer.\n",
      "\n",
      "I'm gonna say a reflectometer.\n",
      "\n",
      "That is correct.\n",
      "\n",
      "Yes.\n",
      "\n",
      "All\n"
     ]
    }
   ],
   "source": [
    "from openai_backTranslation import generate\n",
    "\n",
    "hidden_output = generate(hide_input, max_tokens=len(hide_input))\n",
    "# print(hidden_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016苏州吴中·太湖经贸合作洽谈会成功举办，各大媒体广泛报道\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SecurityXuanwuLab/HaS-820m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"SecurityXuanwuLab/HaS-820m\").to('cuda:0')\n",
    "seek_template = \"Convert the text:\\n%s\\n\\n%s\\n\\nConvert the text:\\n%s\\n\\n\"\n",
    "hide_input = \"前天，'2022北京海淀·颐和园经贸合作洽谈会成功举行，各大媒体竞相报道了活动盛况，小李第一时间将昨天媒体报道情况进行了整理。人民日报 中国青年网 国际联合报 北京商报 消费者观察报 上海晚报 杭州日报 海峡晚报 北京日报 北京市电视一台?北京新闻 人民网 手机雅虎网 网易北京 长三角经济网 新京网 中国农业新闻网 北京圆桌 居然有这么多!还有部分媒体将在未来一周陆续发稿，为经洽会点!为海淀点!阅读投诉阅读精选留言加载中以上留言由公众号筛选后显示了解留言功能详情\"\n",
    "hide_output = \"2022北京海淀·颐和园经贸合作洽谈会成功举办，各大媒体广泛报道\"\n",
    "original_input = \"昨天，’2016苏州吴中·太湖经贸合作洽谈会成功举行，各大媒体竞相报道了活动盛况，小吴第一时间将今天媒体报道情况进行了整理。新华社 中国青年报?中青在线 香港大公报?大公网 香港商报 消费者导报 扬子晚报 江南时报 苏州日报 姑苏晚报 城市商报 苏州广电一套?苏州新闻 新华网 手机凤凰网 网易苏州 长三角城市网 新苏网 中国商务新闻网 苏州圆桌 居然有这么多!还有部分媒体将在今后几天陆续发稿，为经洽会点!为吴中点!阅读投诉阅读精选留言加载中以上留言由公众号筛选后显示了解留言功能详情\"\n",
    "input_text = seek_template % (hide_input, hide_output, original_input)\n",
    "inputs = tokenizer(input_text, return_tensors='pt').to('cuda:0')\n",
    "pred = model.generate(**inputs, max_length=512)\n",
    "pred = pred.cpu()[0][len(inputs['input_ids'][0]):]\n",
    "original_output = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "print(original_output)\n",
    "\n",
    "# output:\n",
    "# 2016苏州吴中·太湖经贸合作洽谈会成功举办，各大媒体广泛报道\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase the following document:\n",
      "The capital of the United States is Washington D.C.\n",
      "Paraphrased document:\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Washington D.C. is the capital of the United States.\n",
      "\n",
      "Q: False\n",
      "A: True or\n",
      "========================================\n",
      "Original Input Prompt:The capital of France is Paris.\n",
      "Hide Input Prompt:The capital of the United States is Washington D.C.\n",
      "Hide Output Prompt:Washington D.C. is the capital of the United States.\n",
      "\n",
      "Q: False\n",
      "A: True or\n",
      "Original Output Prompt:Paris is the capital of France.\n"
     ]
    }
   ],
   "source": [
    "from openai_backTranslation import generate\n",
    "from dpps.HaS import HaS\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "has = HaS(device)\n",
    "original_input = \"华纳兄弟影业（Warner Bro）著名的作品有《蝙蝠侠》系列、《超人》系列、《黑客帝国》系列和《指环王》系列。目前华纳未考虑推出《蝙蝠侠》系列新作。\"\n",
    "hide_input = has.hide(original_input)\n",
    "hide_output = generate(f\"Paraphrase the following document:\\n{hide_input}\\nParaphrased document:\\n\", max_tokens=20)\n",
    "original_output = has.seek(hide_input, hide_output, original_prompt)\n",
    "print(\"Original Input Prompt:\" + original_prompt)\n",
    "print(\"Hide Input Prompt:\" + hide_input)\n",
    "print(\"Hide Output Prompt:\" + hide_output)\n",
    "print(\"Original Output Prompt:\" + original_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
